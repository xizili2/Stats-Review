{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso L1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L1 regularization, also known as Lasso (Least Absolute Shrinkage and Selection Operator) regularization, is a technique used in statistical models, such as linear regression, to prevent overfitting and to perform feature selection. It involves adding a penalty equal to the absolute value of the magnitude of coefficients to the loss function. This has the effect of not only reducing the values of the coefficients to prevent overfitting but also potentially reducing some coefficients to zero, thereby effectively removing those features from the model. This can lead to sparse models when dealing with high-dimensional data, where sparse implies that only a subset of the features contribute to the prediction with non-zero coefficients.\n",
    "\n",
    "### Mathematical Representation\n",
    "\n",
    "In a linear regression model, the ordinary least squares (OLS) method seeks to minimize the residual sum of squares (RSS) to find the best-fitting line. L1 regularization adds a penalty term to the RSS, leading to the following optimization problem:\n",
    "\n",
    "\\[\n",
    "\\text{Minimize: } RSS + \\lambda \\sum_{j=1}^{p} |\\beta_j|\n",
    "\\]\n",
    "\n",
    "where:\n",
    "- \\(RSS\\) is the residual sum of squares,\n",
    "- \\(\\beta_j\\) are the coefficients of the model,\n",
    "- \\(p\\) is the number of features,\n",
    "- \\(\\lambda\\) is a non-negative regularization parameter that controls the strength of the penalty. As \\(\\lambda\\) increases, the penalty for having large coefficients increases, which can drive some coefficients to zero.\n",
    "\n",
    "### Effects of L1 Regularization\n",
    "\n",
    "- **Feature Selection**: By driving some coefficients to zero, L1 regularization performs automatic feature selection, identifying potentially relevant features and discarding irrelevant ones.\n",
    "- **Sparsity**: The resulting model is sparse, which means it uses only a subset of all the features available, making the model simpler and potentially easier to interpret.\n",
    "- **Prevention of Overfitting**: By adding a penalty on the size of the coefficients, L1 regularization helps to reduce the model's complexity, which can prevent overfitting to the training data. This can improve the model's generalization to new, unseen data.\n",
    "\n",
    "### Choosing \\(\\lambda\\)\n",
    "\n",
    "The choice of \\(\\lambda\\) is critical in L1 regularization:\n",
    "- If \\(\\lambda = 0\\), the penalty term has no effect, and the solution is equivalent to the OLS solution.\n",
    "- As \\(\\lambda\\) increases, more coefficients are set to zero, leading to a simpler model.\n",
    "- If \\(\\lambda\\) is too large, it can oversimplify the model, leading to underfitting.\n",
    "\n",
    "The optimal value of \\(\\lambda\\) is usually chosen via cross-validation, balancing the trade-off between bias and variance to achieve good predictive performance on unseen data.\n",
    "\n",
    "### Applications\n",
    "\n",
    "L1 regularization is widely used in machine learning and data science, especially in scenarios with high-dimensional data where feature selection is crucial. It's particularly useful when it's believed that only a small number of features are relevant for predicting the outcome, as it can help identify those features automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In Python, you can apply L1 regularization (Lasso regularization) using libraries such as scikit-learn, which is a popular machine learning library for Python. The Lasso class in scikit-learn provides an easy-to-use implementation of L1 regularization for linear models.\n",
    "\n",
    "Here's a basic example of how to use L1 regularization with a linear regression model in Python using scikit-learn:\n",
    "\n",
    "# Step 1: Install scikit-learn (if necessary)\n",
    "If you haven't installed scikit-learn yet, you can do so using pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.datasets import make_regression\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Create a sample dataset\n",
    "For demonstration purposes, let's generate a regression dataset using scikit-learn's make_regression function:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code generates a dataset with 1000 samples and 20 features, along with some noise to simulate real-world data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Initialize and fit the Lasso model\n",
    "You can specify the strength of the regularization using the alpha parameter; a larger value applies more regularization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Lasso(alpha=0.1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Lasso</label><div class=\"sk-toggleable__content\"><pre>Lasso(alpha=0.1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "Lasso(alpha=0.1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the Lasso regression model with an alpha value\n",
    "lasso = Lasso(alpha=0.1)\n",
    "\n",
    "# Fit the model to the data\n",
    "lasso.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Inspect the coefficients\n",
    "After fitting the model, you can look at the coefficients to see the effect of the L1 regularization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [79.91183621 98.47656042  5.47173655  0.         86.35393606 -0.\n",
      " 69.32137511 -0.         -0.         -0.         18.49633831 39.5328363\n",
      " -0.          2.9809935   0.         26.28106376 -0.         86.78388173\n",
      "  0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(\"Coefficients:\", lasso.coef_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coefficients that are zero indicate features that have been excluded by the Lasso regularization, demonstrating the feature selection capability of L1 regularization.\n",
    "\n",
    "# Choosing alpha\n",
    "The choice of alpha (the regularization strength) is crucial. It controls the level of sparsity of the coefficients estimated by the lasso. A larger alpha means more regularization, leading to sparser coefficients. The best alpha can be found using cross-validation, for which scikit-learn provides LassoCV, a Lasso model fitted with cross-validation to determine the best alpha:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha: 0.0018329807108324356\n",
      "Coefficients with best alpha: [ 7.99998993e+01  9.85773573e+01  5.56550116e+00  1.05886446e-03\n",
      "  8.64640763e+01 -1.80801810e-03  6.94288298e+01 -0.00000000e+00\n",
      "  5.03567617e-04 -1.24509783e-03  1.86046539e+01  3.96336467e+01\n",
      "  0.00000000e+00  3.10114942e+00 -2.45672152e-03  2.63846239e+01\n",
      " -7.98343894e-04  8.68809418e+01  1.86024642e-03  0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "# Initialize the LassoCV model\n",
    "lasso_cv = LassoCV(alphas=np.logspace(-4, 4, 20), cv=5)\n",
    "\n",
    "# Fit the model to the data\n",
    "lasso_cv.fit(X, y)\n",
    "\n",
    "# The best alpha value found\n",
    "print(\"Best alpha:\", lasso_cv.alpha_)\n",
    "\n",
    "# Using the best model\n",
    "print(\"Coefficients with best alpha:\", lasso_cv.coef_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach automatically selects the best alpha from the provided range, using cross-validation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
